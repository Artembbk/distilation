{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 4: уменьшение размеров модели\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __14 баллов__. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн 20.12.23__ \\\n",
    "__Жесткий дедлайн 20.12.23__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит научиться решать задачу Named Entity Recognition (NER) на самом популярном датасете – [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003). В вашем распоряжении будет предобученный BERT, который вам необходимо уменьшить без потерь в качестве. Задание разделено на две части. Первая часть состоит из набора методов по уменьшению модели, которые нужно реализовать по инструкции. Вторая часть – это творческое соревнование, в котором вы можете пользоваться любыми методами, кроме ансамблирования и использования дополнительных данных. Дополнительное условие соревнования: размер вашей модели __не может превышать 20M параметров__.\n",
    "\n",
    "__!!ВАЖНО!!__ Вам придется проводить довольно много экспериментов, поэтому мы рекомендуем не писать весь код в тетрадке, а завести разные файлы для отдельных логических блоков и скомпоновать все в виде проекта. Это позволит вашему ноутбуку не разрастаться и сильно облегчит задачу и вам, и проверяющим.\n",
    "\n",
    "\n",
    "### О датасете\n",
    "\n",
    "В CoNLL-2003 для именования сущностей используется маркировка **BIO** (Beggining, Inside, Outside), в которой метки означают следующее:\n",
    "\n",
    "- *B-{метка}* – начало сущности *{метка}*\n",
    "- *I-{метка}* – продолжнение сущности *{метка}*\n",
    "- *O* – не сущность\n",
    "\n",
    "Существуют так же и другие способы маркировки, например, BILUO. Почитать о них можно [тут](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) и [тут](https://www.youtube.com/watch?v=dQw4w9WgXcQ).\n",
    "\n",
    "Всего в датасете есть 9 разных меток.\n",
    "- O – слову не соответствует ни одна сущность.\n",
    "- B-PER/I-PER – слово или набор слов соответстует определенному _человеку_.\n",
    "- B-ORG/I-ORG – слово или набор слов соответстует определенной _организации_.\n",
    "- B-LOC/I-LOC – слово или набор слов соответстует определенной _локации_.\n",
    "- B-MISC/I-MISC – слово или набор слов соответстует сущности, которая не относится ни к одной из предыдущих. Например, национальность, произведение искусства, мероприятие и т.д.\n",
    "\n",
    "Приступим!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fa6792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/artem/.local/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/artem/.local/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /home/artem/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/artem/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/artem/.local/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/artem/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/artem/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3/dist-packages (from transformers) (4.57.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/artem/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/artem/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab00b846-b353-43a2-9ee8-dc19ca6fd2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c5834-9f37-4cd3-a555-ec9655e02e04",
   "metadata": {},
   "source": [
    "__Задание 1 (0.5 балла)__ Допишите функцию `read_conll2003` для чтения датасета. Внутри она должна проитерироваться по всем строкам файла и для каждого примера составить словарь с полями `words` и `tags` (слова и тэги текста соответственно). На выход функция возвращает список полученных словарей. Тексты в файле разделяются переносом строки `\\n`, а слова и тэги – проблелом. Пример:\n",
    "```\n",
    "! head -n 15 CoNLL2003/train.txt\n",
    "\n",
    "EU B-ORG\n",
    "rejects O\n",
    "German B-MISC\n",
    "call O\n",
    "to O\n",
    "boycott O\n",
    "British B-MISC\n",
    "lamb O\n",
    ". O\n",
    "\n",
    "Peter B-PER\n",
    "Blackburn I-PER\n",
    "\n",
    "BRUSSELS B-LOC\n",
    "1996-08-22 O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c1a5b0-6ca1-4159-9ce6-cff88aca6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll2003(path: str) -> List[Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Read data in CoNNL like format.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        example = {\"words\": [], \"tags\": []}\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, tag = line.split()\n",
    "                example[\"words\"].append(word)\n",
    "                example[\"tags\"].append(tag)\n",
    "            else: \n",
    "                dataset.append(example)\n",
    "                example = {\"words\": [], \"tags\": []}\n",
    "        \n",
    "        if example[\"words\"]:\n",
    "            dataset.append(example)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3281d6-a44d-4680-bb40-de8ba48a686f",
   "metadata": {},
   "source": [
    "Прочитаем тренировочный и валидационный датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b0e34d-edca-40bc-83ac-cff0c2872f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = read_conll2003(\"CoNLL2003/train.txt\")\n",
    "valid_dataset = read_conll2003(\"CoNLL2003/valid.txt\")\n",
    "\n",
    "tags = ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958cdff9-6ea1-4f7f-808b-dbe5620c27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "assert sample['words'] == ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
    "assert sample['tags'] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
    "\n",
    "for w, t in zip(sample['words'], sample['tags']):\n",
    "    print(f'{w}\\t{t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2312e-1335-4afa-a6e6-3cdde8515fe5",
   "metadata": {},
   "source": [
    "На протяжении всего домашнего задания мы будем использовать _cased_ версию BERT, то есть токенизатор будет учитывать регистр слов. Для задачи NER регистр важен, так как имена и названия организаций или предметов искусства часто пишутся с большой буквы, и будет глупо прятать от модели такую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05edcd4e-5360-41a8-b403-a9084d6a3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f64076-829a-49f1-af58-6fe60c66f965",
   "metadata": {},
   "source": [
    "Заметьте, что при токенизации слова могут разделиться на несколько токенов (как слово `lamb` из примера ниже), из-за чего появится несоответствие между числом токенов и тэгов. Это несоответствие нам придется устранить вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6acafc-c94c-4c3a-9c55-82902436ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Токены: ['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sample['words'], is_split_into_words=True)\n",
    "print('Слова: ', sample['words'])\n",
    "print('Токены:', inputs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634984a1-6764-4249-9c8d-4c46f285430c",
   "metadata": {},
   "source": [
    "К счастью, из выхода токенизатора можно достать список с номерами слов, к которым относится каждый токен. Если номер встретился несколько раз подряд, то слово разделилось. Специальные символы не принадлежат никакому слову, поэтому их номер – `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac4fd75-cfef-4b6c-b80e-8d55e6c6349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb3ca9-a8e1-4922-8c2d-dd6371396a9f",
   "metadata": {},
   "source": [
    "__Задание 2 (0.5 балла)__ Допишите метод `get_inputs_and_aligned_labels` класса `Dataset`. Он принимает в себя объект из прочитанного выше датасета, токенизирует слова и выравнивает тэги. Выравнивание происходит следующим образом: если токен пренадлежит тому же слову, что и предыдущий токен, и его тэг начинается на `B`, то надо поменять `B` на `I`, потому что это уже продолжение сущности; в любом другом случае тэг токена остается таким же, какой был у соответствующего ему слова.\n",
    "\n",
    "Метод позвращает словарь с полями `input_ids` – результат токенизации, `labels` – индексы тэгов для каждого токена из маппинга `tag2id`, для специальных символов в качестве лейбла укажите -100, так как это значение по умолчанию, которое игнорируется при подсчете кросс-энтропии в классе `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ebc8789-0bba-4c96-aa1a-84403c93260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, raw_dataset: List[Dict[str, str]], tag2id: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        :params:\n",
    "        raw_dataset: output of read_conll2003 function\n",
    "        tag2id: mapping from tag name to its id\n",
    "        \"\"\"\n",
    "        self.dataset = raw_dataset\n",
    "        self.tag2id = tag2id\n",
    "\n",
    "    def get_inputs_and_aligned_labels(self, sample):\n",
    "        \"\"\"\n",
    "        Aligns tags with tokens and returns dict with token ids and tag ids.\n",
    "        \"\"\"\n",
    "        tokenized = tokenizer(sample['words'], is_split_into_words=True)\n",
    "        tokens = tokenized.tokens()\n",
    "        inpits_ids = tokenized.input_ids\n",
    "        tags = []\n",
    "\n",
    "        tag_i = -1\n",
    "        print(tokens)\n",
    "        print(sample['tags'])\n",
    "        for i in range(len(tokens)):\n",
    "            print(tokens[i])\n",
    "            if tokens[i].startswith(\"##\"):\n",
    "                if sample[\"tags\"][tag_i].startswith(\"B\"):\n",
    "                    tags.append(\"I\" + sample[\"tags\"][tag_i][1:])\n",
    "                else:\n",
    "                    tags.append(sample[\"tags\"][tag_i])\n",
    "            elif tokens[i] in tokenizer.special_tokens_map.values():\n",
    "                tags.append(-100)\n",
    "            else:\n",
    "                tag_i += 1\n",
    "                tags.append(sample[\"tags\"][tag_i])\n",
    "\n",
    "        labels = []\n",
    "        for tag in tags:\n",
    "            if tag == -100:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(self.tag2id[tag])\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': inpits_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        return self.get_inputs_and_aligned_labels(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c04b6ce-6683-453e-acf9-dfbc54ed4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {tag: i for i, tag in enumerate(tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "train_dataset = Dataset(train_dataset, tag2id)\n",
    "valid_dataset = Dataset(valid_dataset, tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6bb3e93-65f7-465c-8f11-48e7fb1846f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "[CLS]\n",
      "EU\n",
      "rejects\n",
      "German\n",
      "call\n",
      "to\n",
      "boycott\n",
      "British\n",
      "la\n",
      "##mb\n",
      ".\n",
      "[SEP]\n",
      "101\t[CLS]\t-100\t\n",
      "7270\tEU\t2\tB-ORG\n",
      "22961\trejects\t8\tO\n",
      "1528\tGerman\t1\tB-MISC\n",
      "1840\tcall\t8\tO\n",
      "1106\tto\t8\tO\n",
      "21423\tboycott\t8\tO\n",
      "1418\tBritish\t1\tB-MISC\n",
      "2495\tla\t8\tO\n",
      "12913\t##mb\t8\tO\n",
      "119\t.\t8\tO\n",
      "102\t[SEP]\t-100\t\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "input_ids, labels = sample['input_ids'], sample['labels']\n",
    "\n",
    "assert input_ids == [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]\n",
    "assert labels == [-100, 2, 8, 1, 8, 8, 8, 1, 8, 8, 8, -100]\n",
    "\n",
    "for idx, token, label in zip(input_ids, tokenizer.convert_ids_to_tokens(input_ids), labels):\n",
    "    tag = id2tag[label] if label != -100 else ''\n",
    "    print(f'{idx}\\t{token}\\t{label}\\t{tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838d966-c241-476e-9016-8ece166efae8",
   "metadata": {},
   "source": [
    "На данный момент наш датасет возвращает по индексу списки токенов и меток, но при формировании батча нам надо их дополнить паддингами. Для этого существует Collator – класс, который вызывается при формировании батча. Он принимает набор произвольных объектов из датасета и делает из них тензоры согласно инструкциям. Для задачи классификации последовательности имеется специальный `DataCollatorForTokenClassification`, который добавляет паддинги к токенам и меткам, что нам собственно и нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6d9171-cd34-4f50-abc1-21af3f1ca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c068d33c-d95d-472e-9e12-f6d13a37d630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "[CLS]\n",
      "EU\n",
      "rejects\n",
      "German\n",
      "call\n",
      "to\n",
      "boycott\n",
      "British\n",
      "la\n",
      "##mb\n",
      ".\n",
      "[SEP]\n",
      "['[CLS]', 'Peter', 'Blackburn', '[SEP]']\n",
      "['B-PER', 'I-PER']\n",
      "[CLS]\n",
      "Peter\n",
      "Blackburn\n",
      "[SEP]\n",
      "Поля:\n",
      " dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Индексы токенов:\n",
      " tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
      "           119,   102],\n",
      "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Индексы меток:\n",
      " tensor([[-100,    2,    8,    1,    8,    8,    8,    1,    8,    8,    8, -100],\n",
      "        [-100,    3,    7, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([train_dataset[i] for i in range(2)])\n",
    "print('Поля:\\n', batch.keys())\n",
    "print('\\nИндексы токенов:\\n', batch['input_ids'])\n",
    "print('\\nИндексы меток:\\n', batch['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a4dc1-722f-4847-ba82-7d7804887edc",
   "metadata": {},
   "source": [
    "Теперь мы готовы обернуть всю нашу красоту в `DataLoader`, по которому будем итерироваться при обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72062a2a-9ef9-427a-8f45-11be27bdb73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9399-8393-4312-88aa-53b727d9df7d",
   "metadata": {},
   "source": [
    "### Метрика\n",
    "\n",
    "Для оценки качества NER чаще всего используется F1-мера. Разделяют два метода подсчета метрики:\n",
    "1) Token-level: считается правильность предсказания отденьной метки для каждого токена.\n",
    "2) Entity-level: считается правильность предсказания метки для всей сущности целиком независимо от того, сколько слов или токенов в нее входит.\n",
    "\n",
    "Обычно предпочтение отдается второму способу, так как иначе, во-первых, качество зависит от токенизации, а во-вторых, если сущность состоит из нескольких слов и модель выставляет словам разные метки, то становится непонятно, к какому именно классу относить данную сущность. Для практики такой результат настолько же плох, насколько полное неугадывание класса, поэтому странно давать за это баллы.\n",
    "\n",
    "Заметьте, предсказание `[I-PER', 'I-PER]` при верном `[B-PER', 'I-PER]` считается корректным, так как из него можно однозначно восстановить ответ, догадавшись, что не первом месте должно стоять `B-`. В то же время при верном `[B-PER', 'B-PER]` такое предсказание корректным не будет.\n",
    "\n",
    "Для подсчета метрики будем использовать уже готовое [решение](https://huggingface.co/spaces/evaluate-metric/seqeval) из библиотеки `seqeval` (семейство `huggingface`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3df54ab-c65b-40e0-b479-25d6f29e5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seqeval in /home/artem/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/artem/.local/lib/python3.10/site-packages (from seqeval) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/artem/.local/lib/python3.10/site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/artem/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/artem/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380833ce-1b8e-4b00-90ee-9126df16c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "434bd7bc-badd-49b1-8024-7f7ff0e92f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5714285714285714, 0.4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are 7 labels in total, we guessed correctly 4 of them.\n",
    "\n",
    "predictions = [['O', 'I-PER', 'I-PER', 'O'], ['I-PER', 'I-PER', 'O']]\n",
    "references = [['O', 'B-PER', 'B-PER', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "acc = accuracy_score(predictions, references)\n",
    "f1 = f1_score(references, predictions)\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948569a7-b918-408d-9e5e-c69cb2ad70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(predictions: List[List[int]], labels: List[List[int]]):\n",
    "    \"\"\"\n",
    "    :params:\n",
    "    predictions: list of lists of predicted labels\n",
    "    labels: list of lists of ground truth labels\n",
    "    \"\"\"\n",
    "    text_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
    "    text_predictions = []\n",
    "    for i in range(len(text_labels)):\n",
    "        # +1 because we skip the first ([CLS]) token\n",
    "        sample_text_preds = [id2tag[predictions[i][j + 1]] for j in range(len(text_labels[i]))]\n",
    "        text_predictions.append(sample_text_preds)\n",
    "\n",
    "    return f1_score(text_labels, text_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61400bf-712a-4dfb-a08f-326c5db10eb2",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "В качестве начальной модели мы будем использовать предобученный BERT, а если быть точнее `bert-base-cased` из библиотеки `huggingface`. Он содержит 107М параметров. В последующих заданиях мы будем реализовывать методы для уменьшения его размеров с минимальной потерей качества.\n",
    "\n",
    "Для классификации последовательностей в `transformers` существует специальная обертка `AutoModelForTokenClassification`. Воспользуемся ею и обернем нашу модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14235f7c-d7a3-4407-98fe-be35bec84008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_67156/2686100005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of parameters:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2458\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m                 )\n\u001b[0;32m-> 2460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', id2label=id2tag, label2id=tag2id).to(device)\n",
    "print('Number of parameters:', sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3fb2f-c76a-4070-bbea-be130243f18b",
   "metadata": {},
   "source": [
    "## Обучение всякого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11672838-d407-448c-96ab-359559a46b6b",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл)** Все методы уменьшения размерности основываются на том, что у нас есть некоторая обученная модель. Сейчас у нас есть предобученный BERT, но на задачу MLM, а не NER. Дообучите BERT на нашем датасете. Ориентировочно у вас должно получиться значение F1 не меньше 0.93 на валидационной выборке. Само обучение никак не должно занимать больше получаса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758fa30-9a34-4fd2-b59f-51cce62eb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc9778-e7ce-4b45-88dd-210837be67d4",
   "metadata": {},
   "source": [
    "### Embedding factorization\n",
    "\n",
    "Можно заметить, что на данный момент матрица эмбеддингов занимает $V \\cdot H = 28996 \\cdot 768 = 22.268.928$ параметров. Это целая пятая часть от всей модели! Давайте попробуем с этим что-то сделать. В вариации [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) предлагается факторизовать матрицу эмбеддингов в произведение двух небольших матриц. Таким образом, параметры эмбеддингов будут содержать $V \\cdot E + E \\cdot H$ элементов, что гораздо меньше, если $H \\gg E$. Авторы выбирают $E = 128$, однако ничего не мешает вам взять значение меньше.\n",
    "\n",
    "__Задание 4 (1 балл)__ Замените слой эмбеддингов на описанную факторизацию и дообучите полученную в предыдущем задании модель. Насколько вам удалось уменьшить число параметров? Если вы все сделали правильно, то F1-мера на валидации не должна опуститься ниже 0.9.\n",
    "\n",
    "Мы настоятельно рекомендуем переиспользовать код для обучения из предыдущего задания и не создавать новую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf027f56-ab30-4691-8c07-ed83d41b45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### Дистилляция знаний\n",
    "\n",
    "Дистилляция знаний – это парадигма обучения, в которой знания модели-учителя дистиллируются в модель-ученика. Учеником может быть произвольная модель меньшего размера, решающая ту же задачу. При дистилляции используются два функционала ошибки:\n",
    "\n",
    "1. Стандартная кросс-энтропия.\n",
    "1. Функция, задающая расстояние между распределениями предсказаний учителя и ученика. Чаще всего используют кросс-энтропию или KL-дивергенцию.\n",
    "\n",
    "При этом для того, чтобы распределение предсказаний учителя не было таким вырожденным используют softmax с температурой больше 1, например, 2 или 5.\n",
    "\n",
    "<img src=\"https://intellabs.github.io/distiller/imgs/knowledge_distillation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Задание 5 (1 балл)__ Реализуйте метод дистилляции знаний, изображенный на картинке. Для подсчета ошибки между предсказаниями ученика и учителя используйте KL-дивергенцию (`nn.KLDivLoss(reduction=\"batchmean\")`). В качестве учителя используйте дообученный BERT из задания 3. В качестве ученика вы можете взять произвольную необученную модель с размером около 40M параметров. Не забудьте про warmup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f71d61-bb55-49d1-a77a-165476d6daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878a3da-a938-4a6f-87a5-9ebc8f81647a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Соревнование (до 10 баллов)\n",
    "\n",
    "Ваша задача – обучить модель с размером __не больше 20М параметров__ для задачи NER. При этом можно пользоваться предобученным `bert-base-cased`, но больше ничем. \n",
    "\n",
    "Соревнование будет проходить аналогично соревнованию из второго домашнего задания. Ваши посылки вы должны будете отправлять тг-боту [@nlp_hw4_bot](t.me/nlp_hw4_bot), а он будет считать значения F1 на публичном и приватном датасетах и записывать результат в [табличку](https://docs.google.com/spreadsheets/d/1rILRI16VxgztwlfqR2kPZ3MxlJkerz6iEr5Kx9yrOLA/edit#gid=0).\n",
    "\n",
    "Для формирования посылки вам нужно будет создать папку на dropbox, положить в нее файл `model.py` с классом модели `Model` и веса `weights.pt`, а затем отправить боту ссылку на эту папку, доступную к чтению. Бот будет импортировать модель и загружать веса:\n",
    "```\n",
    "module = __import__('model', globals(), locals(), ['Model'], 0)\n",
    "model = module.Model()\n",
    "model.load_state_dict(torch.load('weights.pt', map_location=torch.device('cpu')))\n",
    "```\n",
    "\n",
    "При тестировании модель будет получать на вход `input_ids` и `attention_mask`, а на выход должна возвращать трехмерный тензор с вероятностями меток для каждого токена в батче. Класс `Model` должен содержать атрибут `id2label` совпадающий с тем, который задан в конфиге модели `model.config.id2label`. Это нужно для того, чтобы id тэгов мапились в нужные названия тэгов, так как они могут отличаться у разных решений.\n",
    "\n",
    "\n",
    "__Обязательм условием__ участия в соревновании является отчет о проделанной работе в формате pdf, в котором вы должны описать опробованные методы с результатами. За отчет выставляется максимум до __2 баллов__ на усмотрение проверяющего. В случае отсутствия отчета баллы за соревнование __обнуляются__.\n",
    "\n",
    "После дедлайна по домашке будут выложен _приватный_ лидерборд, по которому и будут выставляться баллы за соревнования. За место в лидерборде можно получить до __8__ баллов, но только при условии, если вы получили больше __0.8__ на _публичном_ лидерборде, в противном случае баллы выставляться не будут.\n",
    "$$\n",
    "\\text{число баллов} = 8\\frac{(N - r + 1)}{N},\n",
    "$$\n",
    "где $r$ – место в лидерборде, а $N$ - число участников со значением F1 на _публичном_ лидерборде не меньше __0.8__.\n",
    "\n",
    "\n",
    "На сервере установлена версия библиотеки `transformers==4.34.0`.\n",
    "В разных версиях может отличаться вид хранения весов, поэтому рекомендуем установить себе такую же версию, чтобы избежать ошибок при загрузке модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8611d83-7b80-4906-b5f9-bb30b2c1c5a8",
   "metadata": {},
   "source": [
    "### Что стоит попробовать?\n",
    "\n",
    "* В статье [ALBERT](https://arxiv.org/abs/1909.11942) помимо факторизации эмбеддингов предлагается использовать одни и те же параметры для нескольких слоев. Такой подход позволяет серьезно уменьшить число параметров.\n",
    "\n",
    "* В задании 5 мы инициализировали ученика случайно, однако можно сделать лучше. При дистилляции знаний для downstream задачи из предобученного в unsupervised формате учителя (задача MLM) часто помогает сперва дистиллировать модель для задачи предобучения, а затем ее уже дообучать на downstream задачу с соответствующим учителем. Другими словами, лучше сначала дистиллировать предобученный BERT в ученика на MLM задаче, а затем использовать этого ученика в качестве начальной инициализации для второй дистилляции.\n",
    "\n",
    "* При дистилляции мы выравниваем только предсказания моделей, однако можно выравнивать еще и скрытые слои. Например, приближать матрицы внимания и выходы каждого скрытого слоя. Подробнее об этом можно почитать [тут](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT).\n",
    "\n",
    "* В данный момент мы используем все головы внимания, но ряд исследований показывает, что большинство из них можно выбросить без потери качества. В этой [статье](https://arxiv.org/pdf/1905.09418.pdf) предлагается следующий подход. Добавим гейт $g_i \\in \\{0, 1\\}$ для каждой головы внимания.\\\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_I^V)\n",
    "$$\n",
    "$$\n",
    "\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(g_i \\cdot \\text{head}_i) W^O\n",
    "$$\n",
    "---\n",
    "__Теоретический блок для тех, кому интересно__   \n",
    "Будем настраивать значения гейтов в процессе обучения. Мы хотим, чтобы как можно большая часть гейтов стала нулями, поэтому добавим в функционал модели $L_0$ регуляризацию на $g_i$. Проблема в том, что $L_0$ – недифференцируемая функция, поэтому нам надо релаксировать ее.\n",
    "Будем считать, что каждый гейт задается распределением Бернулли, $g_i \\sim \\text{Bernoulli}(\\alpha_i)$, где $\\alpha_i$ – настраиваемый параметр. Осталось понять, как его настраивать. Если мы будем напрямую семплировать из распределения Бернулли, то мы потеряем связь между $\\alpha_i$ и семплом, поэтому мы не сможем прокинуть градиенты (такая же проблема возникает в VAE, там используют reparametrization trick). Существует хороший способ семплирования дискретных случайных величин с сохранением связи – __Gumbel-Max trick__. Пусть вероятность каждого значения случайной величины пропорциональна $\\beta_k \\in (0, \\infty)$ и \\\n",
    "$$\n",
    "x = \\text{argmax}_k \\{\\log \\beta_k - \\log(-\\log(\\text{Uniform}(0, 1))) \\},\n",
    "$$\n",
    "Тогда $P(x = k) = \\frac{\\beta_k}{\\sum_k \\beta_k}$. Для того, чтобы избавиться от недифференцируемого аргмакса можно релаксировать его, заменив на softmax с температурой меньше 1, так мы получим [Concrete distribution](https://arxiv.org/pdf/1611.00712.pdf). Теперь мы сможем в процессе обучения семплировать значения гейтов и обновлять $\\alpha_i$ градиентным спуском. Почти победа, осталось понять, при каких значениях $\\alpha_i$ после обучения мы будем считать, что гейт закрыт и голову можно выбросить. В [статье](https://arxiv.org/pdf/1712.01312.pdf) про Hard Concrete распределение, предложившей $L_0$ регуляризацию, предлагается немного растянуть распределение вероятности открытия гейта с $[0, 1]$ до $[\\gamma, \\zeta]$ (например, $[-0.1, 1.1]$), а затем обрезать его обратно до $[0, 1]$. Таким образом, все значения, которые были меньше 0, превратятся в 0. Теперь мы будем считать гейт закрытым, если мы получили на выходе 0.\n",
    "$$g_i = \\text{clip}\\big(\\text{Sigmoid}(\\log \\alpha_i)(\\zeta - \\gamma) + \\gamma, 0, 1\\big)$$\n",
    "---\n",
    "Получаем следующий алгоритм подбора значений для гейтов.\n",
    "1. Заводим параметр $\\log \\alpha_i$ для каждой головы каждого слоя.\n",
    "2. Добавляем к функционалу ошибки слагаемое регуляризации с небольшим коэффициентом $\\lambda$ (например, $0.02$)\n",
    "$$\n",
    "\\mathcal{L}_C = \\sum_{i=1}^h (1 - P(g_i = 1 | \\alpha_i)) = \\sum_{i=1}^h \\text{Sigmoid}\\Big(\\log \\alpha_i - \\tau \\log \\frac{-\\gamma}{\\zeta}\\Big),\n",
    "$$\n",
    "где $\\gamma$ < 0 и $\\zeta$ > 1, $\\tau$ – гиперпараметры (можно взять $-0.1$, $1.1$ и $0.33$ соответственно)\n",
    "3. При каждом вызове модели значения гейтов семплируются из Hard Concrete распределения.\n",
    "\\begin{align}\n",
    "u &= \\text{Uniform}(0, 1) \\\\\n",
    "z &= \\text{Sigmoid}\\big((\\log u - \\log(1 - u) + \\log \\alpha_i) \\,/\\, \\tau\\big) \\\\\n",
    "g_i &= \\text{clip}\\big( z (\\zeta - \\gamma) + \\gamma, 0, 1 \\big)\n",
    "\\end{align}\n",
    "4. После обучения в идеале выбрасываются головы, для которых $\\text{clip}\\big(\\text{Sigmoid}(\\log \\alpha_i)(\\zeta - \\gamma) + \\gamma, 0, 1\\big)$ равняется 0. Если таких нет или очень мало, то можно выбросить те, которые близки к нулю, а затем дообучить модель без этих голов.\n",
    "\n",
    "\n",
    "[Тут](https://arxiv.org/pdf/2110.03252.pdf) можно почитать про дополнительные хаки для этого метода.\\\n",
    "\\\n",
    "P. S. Заводится тяжело, но заводится. Если гейты не начинают зануляться, то, возможно, вы недостаточно долго учите.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6ebb1-aade-4464-bc57-60607d9434e2",
   "metadata": {},
   "source": [
    "Помимо всего, что написано выше, на просторах интернета можно найти кучу других способов уменьшения модели, не стестяйтесь их искать. При проведении экспериментов старайтесь делать одно изменение за раз и не бросайтесь реализовывать сложный метод, если у вас нет достаточных оснований полагать, что он даст значительный буст.\n",
    "\n",
    "Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460953f-77ae-47b1-b843-edee202e70bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
